#!/bin/bash

#PBS -A V_AC
#PBS -l walltime=V_WT
#PBS -l select=V_NN:ncpus=V_NC:mem=V_MM
#PBS -q parallel

## Copyright 2015-2016 Alessandro Maria Rizzi
##
## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##     http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

## Environment configuration
export MAX_WALLTIME="V_WT"
export NUM_NODES="V_NN"
export NUM_CPUS="V_NC"
export MEM="V_MM"
export QUERIES="V_QR"
export SCALE="V_SC"
export ITERATIONS="N_IT"

module load profile/advanced hadoop/2.5.1

function host_name {
  echo $1 | awk -F. '{ print $1 }'
}

export SCRIPT_PATH="V_SP"
. ${SCRIPT_PATH}/config
export PATH=$HIVE_HOME/bin:$PATH
export HADOOP_USER_CLASSPATH_FIRST=true

export LOG_DIR="$BASE_LOG_DIR/${NUM_NODES}_${NUM_CPUS}_${MEM}_${SCALE}/${PBS_JOBID}"
export QUERY_DIR="$SCRIPT_PATH/queries"

mkdir -p $LOG_DIR
cd $LOG_DIR

echo $MAX_WALLTIME > "$LOG_DIR/maxWallTime"
echo $PBS_JOBID > "$LOG_DIR/id"

rm -f $HOST_FILE


### Get the list of available nodes
if [ -e "$PBS_NODEFILE" ]; then
    cp $PBS_NODEFILE $LOG_DIR/nodes
fi

export MYHADOOP_HOME=$SCRIPT_PATH/myhadoop
echo "Conf: $HADOOP_CONF_DIR"
# Configure a new HADOOP instance using PBS job information
$MYHADOOP_HOME/bin/myhadoop-configure.sh -c $HADOOP_CONF_DIR
# Start the Datanode, Namenode, and the Job Scheduler 
$HADOOP_HOME/sbin/start-dfs.sh
$HADOOP_HOME/sbin/start-yarn.sh

echo "Clean up..."
hdfs dfs -rm -r -f /tmp
hdfs dfs -rm -r -f $DB_DIR/$SCALE
echo "Creating database..."
STARTTIME=$(date +%s)
hdfs dfs -mkdir -p $DB_DIR
$SCRIPT_PATH/copyDb.sh $DB_DIR $DB_LOCAL_DIR/$SCALE
ENDTIME=$(date +%s)
echo "Database copied in $(($ENDTIME - $STARTTIME)) seconds"

echo "Populating db..."
STARTTIME=$(date +%s)
$SCRIPT_PATH/initHive.sh $SCALE $DB_DIR
ENDTIME=$(date +%s)
echo "Database populated in $(($ENDTIME - $STARTTIME)) seconds"


for QUERY in $QUERIES; do
    i=0
    while [ $i -lt $ITERATIONS ]; do
        let i++
        echo "Launching query $QUERY iteration $i..."
        STARTTIME=$(date +%s)
        $SCRIPT_PATH/runAndFetchSingle.sh $QUERY $SCALE $QUERY_DIR $LOG_DIR/hive/$QUERY/$i
        mkdir -p $LOG_DIR/hdfs/$QUERY/$i
        ENDTIME=$(date +%s)
        echo "Query performed in $(($ENDTIME - $STARTTIME)) seconds"
        hdfs dfs -copyToLocal /tmp/hadoop-yarn/staging $LOG_DIR/hdfs/$QUERY/$i
        hdfs dfs -rm -r -f /tmp/hadoop-yarn/staging
    done   
done

# Stop HADOOP services
$HADOOP_HOME/sbin/stop-yarn.sh
$HADOOP_HOME/sbin/stop-dfs.sh

$MYHADOOP_HOME/bin/myhadoop-cleanup.sh > /dev/null

mkdir -p $LOG_DIR/logs
$SCRIPT_PATH/logExtract.sh $LOG_DIR

