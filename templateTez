#!/bin/bash

#PBS -A PUT_HERE_YOUR_CINECA_ACCOUNT
#PBS -l walltime=V_WT
#PBS -l select=V_NN:ncpus=V_NC:mem=V_MM
#PBS -q parallel

## Copyright 2015-2016 Alessandro Maria Rizzi
##
## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##     http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

## Environment configuration
export MAX_WALLTIME="V_WT"
export NUM_NODES="V_NN"
export NUM_CPUS="V_NC"
export MEM="V_MM"
export QUERIES="V_QR"
export SCALE="V_SC"
export ITERATIONS="N_IT"

module load profile/advanced hadoop/2.5.1

function host_name {
  echo $1 | awk -F. '{ print $1 }'
}

export SCRIPT_PATH="V_SP"
. ${SCRIPT_PATH}/config
export PATH=$HIVE_HOME/bin:$PATH
export HADOOP_USER_CLASSPATH_FIRST=true

export LOG_DIR="$BASE_LOG_DIR/${NUM_NODES}_${NUM_CPUS}_${MEM}_${SCALE}/${PBS_JOBID}"
export QUERY_DIR="$SCRIPT_PATH/queries"

mkdir -p $LOG_DIR
cd $LOG_DIR

export TEZ_JARS=$LOG_DIR/tez_jars
export TEZ_CONF_DIR="${SCRIPT_PATH}/tez-conf"
mkdir -p $TEZ_JARS
mkdir -p $TEZ_CONF_DIR



echo $MAX_WALLTIME > "$LOG_DIR/maxWallTime"
echo $PBS_JOBID > "$LOG_DIR/id"

rm -f $HOST_FILE


### Get the list of available nodes
if [ -e "$PBS_NODEFILE" ]; then
    cp $PBS_NODEFILE $LOG_DIR/nodes
fi

export MYHADOOP_HOME=$SCRIPT_PATH/myhadoop

# Configure a new HADOOP instance using PBS job information
$MYHADOOP_HOME/bin/myhadoop-configure.sh -c $HADOOP_CONF_DIR
# Start the Datanode, Namenode, and the Job Scheduler 
$HADOOP_HOME/sbin/start-dfs.sh
$HADOOP_HOME/sbin/start-yarn.sh

hdfs dfs -mkdir -p /apps/tez-${TEZ_VERSION}
hdfs dfs -copyFromLocal $TEZ_DIR/tez-dist/target/tez-${TEZ_VERSION}.tar.gz /apps/tez-${TEZ_VERSION}/
tar -xvzf $TEZ_DIR/tez-dist/target/tez-$TEZ_VERSION-minimal.tar.gz -C $TEZ_JARS
export HADOOP_CLASSPATH=${TEZ_CONF_DIR}:${TEZ_JARS}/*:${TEZ_JARS}/lib/*

echo "Clean up..."
hdfs dfs -rm -r -f /tmp
hdfs dfs -rm -r -f $DB_DIR/$SCALE
echo "Creating database..."
STARTTIME=$(date +%s)
hdfs dfs -mkdir -p $DB_DIR
$SCRIPT_PATH/copyDb.sh $DB_DIR $DB_LOCAL_DIR/$SCALE
ENDTIME=$(date +%s)
echo "Database copied in $(($ENDTIME - $STARTTIME)) seconds"

echo "Populating db..."
STARTTIME=$(date +%s)
$SCRIPT_PATH/initHive.sh $SCALE $DB_DIR
ENDTIME=$(date +%s)
echo "Database populated in $(($ENDTIME - $STARTTIME)) seconds"

mkdir -p $LOG_DIR/mr 

for QUERY in $QUERIES; do
    i=0
    while [ $i -lt $ITERATIONS ]; do
        let i++
        echo "Launching query $QUERY iteration $i..."
        STARTTIME=$(date +%s%N)
        $SCRIPT_PATH/runAndFetchSingleTez.sh $QUERY $SCALE $QUERY_DIR $LOG_DIR/hive/$QUERY/$i
        mkdir -p $LOG_DIR/hdfs/$QUERY/$i
        ENDTIME=$(date +%s%N)
        echo "Query performed in $(($ENDTIME - $STARTTIME)) seconds"
#	sleep 60
	while read -r line; do
	    if [[ $line =~ .*(application_[0-9]+_[0-9]+).* ]]; then
                strresult=${BASH_REMATCH[1]}
	        echo $strresult
		SRC="/pico/scratch/userexternal/$USER/hadoop-$PBS_JOBID/logs/userlogs/$strresult"
		DEST="${LOG_DIR}/mr/$QUERY"
		mkdir -p  ${DEST}
		for f in $(ls ${SRC} | grep container); do
		    cat $SRC/$f/syslog_dag_* >> $DEST/$strresult.AMLOG.txt
              	    echo "" >> $DEST/$strresult.AMLOG.txt
              	    for g in $(ls $SRC/$f | grep syslog_attempt_); do
                 	cat $SRC/$f/$g >> $DEST/$strresult.AMLOG.txt
                	echo "" >> $DEST/$strresult.AMLOG.txt
              	    done
    		done    		
                echo "Finished app: $strresult"
                echo "$strresult" >> "${LOG_DIR}/apps.tmp"
	        mkdir -p  ${LOG_DIR}/mr/queries
	        echo "$strresult" >> "${LOG_DIR}/mr/queries/$QUERY"
	        echo "$((($ENDTIME - $STARTTIME)/1000000))	${strresult}" >> ${LOG_DIR}/mr/$QUERY/appDuration.txt
	        mkdir -p ${LOG_DIR}/$QUERY
                echo "${strresult}\n${STARTTIME}\t${ENDTIME}">> ${LOG_DIR}/$QUERY/real_start_end.txt
                break
            fi
        done < "$LOG_DIR/hive/$QUERY/$i/scratch/temp.tmp"
    done   
done


# Stop HADOOP services
$HADOOP_HOME/sbin/stop-yarn.sh
$HADOOP_HOME/sbin/stop-dfs.sh

$MYHADOOP_HOME/bin/myhadoop-cleanup.sh > /dev/null
# > /dev/null

#mkdir -p $LOG_DIR/tez-logs
#cp -R $CINECA_SCRATCH/hadoop-$PBS_JOBID/conf/logs/userlogs $LOG_DIR/mr
#${SCRIPT_PATH}/tezExtract.sh ${LOG_DIR}/mr

