#!/bin/bash

#PBS -A V_AC
#PBS -l walltime=V_WT
#PBS -l select=V_NN:ncpus=V_NC:mem=V_MM
#PBS -q parallel

## Copyright 2015-2016 Alessandro Maria Rizzi
##
## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##     http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

## Environment configuration
export MAX_WALLTIME="V_WT"
export NUM_NODES="V_NN"
export NUM_CPUS="V_NC"
export MEM="V_MM"
export INPUT_FILE="V_IF"
export SCALE="V_SC"

module load profile/advanced hadoop/2.5.1



function host_name {
  echo $1 | awk -F. '{ print $1 }'
}

export SCRIPT_PATH="V_SP"
. ${SCRIPT_PATH}/config
export PATH=$HIVE_HOME/bin:$PATH

export HADOOP_USER_CLASSPATH_FIRST=true

export LOG_DIR="$BASE_LOG_DIR/${NUM_NODES}_${NUM_CPUS}_${MEM}_${SCALE}/${PBS_JOBID}"
export QUERY_DIR="$SCRIPT_PATH/queries"
export INPUT="$LOG_DIR/ssdata.conf"


mkdir -p $LOG_DIR
cd $LOG_DIR
cat $INPUT_FILE > $INPUT

echo $MAX_WALLTIME > "$LOG_DIR/maxWallTime"
echo $PBS_JOBID > "$LOG_DIR/id"

rm -f $HOST_FILE



### Get the list of available nodes
if [ -e "$PBS_NODEFILE" ]; then
    cp $PBS_NODEFILE $LOG_DIR/nodes
fi

export MYHADOOP_HOME=$SCRIPT_PATH/myhadoop

# Configure a new HADOOP instance using PBS job information
$MYHADOOP_HOME/bin/myhadoop-configure.sh -c $HADOOP_CONF_DIR
# Start the Datanode, Namenode, and the Job Scheduler 
$HADOOP_HOME/sbin/start-dfs.sh
$HADOOP_HOME/sbin/start-yarn.sh

echo "Clean up..."
hdfs dfs -rm -r -f /tmp
hdfs dfs -rm -r -f $DB_DIR/$SCALE
echo "Creating database..."
STARTTIME=$(date +%s)
hdfs dfs -mkdir -p $DB_DIR
$SCRIPT_PATH/copyDb.sh $DB_DIR $DB_LOCAL_DIR/$SCALE
ENDTIME=$(date +%s)
echo "Database copied in $(($ENDTIME - $STARTTIME)) seconds"


echo "Launching iterations..."
i=0
while read line; do
    echo "Launching iteration $line..."
    "${SCRIPT_PATH}/runSingleQueue.sh" ${line} $SCALE $QUERY_DIR $LOG_DIR/hive $i &
    let i++
done < $INPUT

#while [ 1 -eq 1 ]; do
sleep V_RT
#done

mkdir -p $LOG_DIR/hdfs
hdfs dfs -copyToLocal /tmp/hadoop-yarn/staging $LOG_DIR/hdfs


# Stop HADOOP services
$HADOOP_HOME/sbin/stop-yarn.sh
$HADOOP_HOME/sbin/stop-dfs.sh

$MYHADOOP_HOME/bin/myhadoop-cleanup.sh > /dev/null

mkdir -p $LOG_DIR/logs
$SCRIPT_PATH/sessionLogExtract.sh $LOG_DIR

